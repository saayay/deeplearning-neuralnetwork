{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning Assignment 1\n",
    "Due on Thursday, Feb 8, 11:59pm\n",
    "\n",
    "This assignment can be done in groups of at most 2 students. Everyone must submit on Courseworks individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the UNIs of your group (if applicable)\n",
    "\n",
    "Member 1: Ryan Brand, rmb2208\n",
    "\n",
    "Member 2: Saaya Yasuda, sy2569"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports\n",
    "\n",
    "# for data augmentation\n",
    "from scipy.ndimage.interpolation import shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0, momentum=0.0, decay_rate=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dimensions)\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.momentum = momentum\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        # init parameters\n",
    "        for l in range(1, self.num_layers):\n",
    "            n_in = layer_dimensions[l-1]\n",
    "            n_out = layer_dimensions[l]\n",
    "            xavier = 2.0/np.float(n_in + n_out)\n",
    "            \n",
    "            W = xavier*np.random.randn(n_out, n_in)\n",
    "            b = np.zeros((n_out, 1))\n",
    "            \n",
    "            self.parameters[\"W\" + str(l)] = W\n",
    "            self.parameters[\"b\" + str(l)] = b\n",
    "            \n",
    "            if self.momentum > 0 and self.decay_rate > 0:\n",
    "                vW = np.zeros((n_out, n_in))\n",
    "                vb = np.zeros((n_out, 1))\n",
    "                sW = np.zeros((n_out, n_in))\n",
    "                sb = np.zeros((n_out, 1))\n",
    "                \n",
    "                self.parameters[\"vW\" + str(l)] = vW\n",
    "                self.parameters[\"vb\" + str(l)] = vb\n",
    "                self.parameters[\"sW\" + str(l)] = sW\n",
    "                self.parameters[\"sb\" + str(l)] = sb\n",
    "            \n",
    "            elif self.momentum > 0:\n",
    "                vW = np.zeros((n_out, n_in))\n",
    "                vb = np.zeros((n_out, 1))\n",
    "                \n",
    "                self.parameters[\"vW\" + str(l)] = vW\n",
    "                self.parameters[\"vb\" + str(l)] = vb\n",
    "                \n",
    "            elif self.decay_rate > 0:\n",
    "                sW = np.zeros((n_out, n_in))\n",
    "                sb = np.zeros((n_out, 1))\n",
    "                \n",
    "                self.parameters[\"sW\" + str(l)] = sW\n",
    "                self.parameters[\"sb\" + str(l)] = sb\n",
    "        \n",
    "        ##############################\n",
    "        self.debug = False\n",
    "        ##############################\n",
    "        \n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "        Z = np.dot(W, A) + b\n",
    "        cache_layer = (A, W, b, Z)\n",
    "        \n",
    "        return Z, cache_layer\n",
    "    \n",
    "\n",
    "    def activationForward(self, Z, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        \n",
    "        return eval(\"self.\" + activation)(Z)\n",
    "    \n",
    "\n",
    "    def relu(self, X):\n",
    "        \n",
    "        return X*(X>0)\n",
    "    \n",
    "    \n",
    "    def softmax(self, X):\n",
    "        \n",
    "        X_exp = np.exp(X - np.max(X))\n",
    "\n",
    "        return X_exp / np.sum(X_exp, axis=0, keepdims=True)\n",
    "    \n",
    "            \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: \n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        M = np.random.rand(A.shape[0], A.shape[1])\n",
    "        M = (M > prob) * 1.0\n",
    "        M /= (1 - prob)\n",
    "        A *= M\n",
    "        return A, M\n",
    "    \n",
    "\n",
    "    def forwardPropagation(self, X, training=True):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        cache = {}\n",
    "        \n",
    "        A = X\n",
    "        L = self.num_layers-1\n",
    "        for l in range(1, L):\n",
    "            W = self.parameters[\"W\" + str(l)]\n",
    "            b = self.parameters[\"b\" + str(l)]\n",
    "            Z, cache_layer = self.affineForward(A, W, b) # cache_layer is (A, W, b, Z)\n",
    "            A = self.activationForward(Z)\n",
    "            \n",
    "            # dropout\n",
    "            M = np.zeros(A.shape)\n",
    "            if self.drop_prob > 0 and training:\n",
    "                A, M = self.dropout(A, self.drop_prob)\n",
    "            cache[str(l)] = cache_layer + (M,) # cache_layer is (A, W, b, Z, M)\n",
    "                \n",
    "        \n",
    "        WL = self.parameters[\"W\" + str(L)]\n",
    "        bL = self.parameters[\"b\" + str(L)]\n",
    "        ZL, cache_layer = self.affineForward(A, WL, bL) # cache_layer is (A, W, b, Z)\n",
    "        \n",
    "        # dropout - adding a fake M to keep the len(cache[l]) the same.\n",
    "        M = np.zeros(A.shape)\n",
    "        cache[str(L)] = cache_layer + (M,) # cache_layer is (A, W, b, Z, M)\n",
    "        \n",
    "        AL = self.activationForward(ZL, activation=\"softmax\")            \n",
    "\n",
    "        return AL, cache\n",
    "    \n",
    "    def costFunction(self, AL, y, reg_type = None):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param reg_type: regularization type. L1 or L2.\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute loss\n",
    "        S = np.float(AL.shape[1])\n",
    "        Y = one_hot(y)\n",
    "        cost = -np.sum(np.multiply(Y, np.log(AL))) / S\n",
    "        \n",
    "        if self.debug:\n",
    "            print \"########### cost pre-regularization: \", cost\n",
    "        \n",
    "        # Reference for formula: \n",
    "        # L1: http://neuralnetworksanddeeplearning.com/chap3.html#other_techniques_for_regularization\n",
    "        # L2: http://neuralnetworksanddeeplearning.com/chap3.html#regularization\n",
    "        \n",
    "        if self.reg_lambda > 0 and reg_type is not None:\n",
    "            \n",
    "            weight_sum = 0\n",
    "            \n",
    "            if reg_type == \"L1\":\n",
    "                for l in range(1, self.num_layers):\n",
    "                    w_l = self.parameters[\"W\" + str(l)]\n",
    "                    weight_sum += np.abs(w_l).sum() / w_l.shape[1]\n",
    "\n",
    "            elif reg_type == \"L2\":\n",
    "                for l in range(1, self.num_layers):\n",
    "                    w_l = self.parameters[\"W\" + str(l)]\n",
    "                    weight_sum += np.sum(w_l ** 2) / (2 * w_l.shape[1])                \n",
    "\n",
    "            cost += weight_sum * self.reg_lambda\n",
    "            \n",
    "            if self.debug:\n",
    "                print \"########### cost POST-regularization: \", cost\n",
    "        \n",
    "        # gradient of cost\n",
    "        dZL = np.subtract(AL, Y) #10x100\n",
    "        \n",
    "        return cost, dZL\n",
    "\n",
    "    def affineBackward(self, dZ, cache_layer):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :param reg_type: regularization type. L1 or L2.\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        A_prev, W, b, Z, M = cache_layer\n",
    "        S = np.float(A_prev.shape[1])\n",
    "        \n",
    "        dA_prev = np.dot(W.transpose(), dZ)\n",
    "        dW = np.dot(dZ, A_prev.transpose())\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "\n",
    "    def activationBackward(self, dA, cache_layer, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        A_prev, W, b, Z, M = cache_layer\n",
    "        dZ = np.multiply(dA, self.relu_derivative(Z))\n",
    "        \n",
    "        return dZ\n",
    "\n",
    "        \n",
    "    def relu_derivative(self, X):\n",
    "        \n",
    "        return 1.0*(X>0)\n",
    "    \n",
    "\n",
    "    def dropout_backward(self, dZ, cache_layer):\n",
    "        \n",
    "        # Reference:\n",
    "        # https://stats.stackexchange.com/questions/219236/dropout-forward-prop-vs-back-prop-in-machine-learning-neural-network\n",
    "        # https://stats.stackexchange.com/questions/207481/dropout-backpropagation-implementation\n",
    "        \n",
    "        _, _, _, _, M = cache_layer\n",
    "        dZ = dZ * M\n",
    "\n",
    "        return dZ\n",
    "    \n",
    "\n",
    "    def backPropagation(self, dZL, Y, cache, reg_type = None):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :param reg_type: regularization type. L1 or L2.\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        \n",
    "        L = self.num_layers-1\n",
    "        cache_layer = cache[str(L)]\n",
    "        dA, dWL, dbL = self.affineBackward(dZL, cache_layer)\n",
    "        \n",
    "        # L1/L2\n",
    "        if self.reg_lambda > 0 and reg_type is not None:\n",
    "            w_l= cache_layer[1]\n",
    "            if reg_type == \"L1\":\n",
    "                dWL += np.sign(w_l) * (self.reg_lambda / w_l.shape[1])\n",
    "            elif reg_type == \"L2\":\n",
    "                dWL += w_l * (self.reg_lambda / w_l.shape[1])\n",
    "                \n",
    "        gradients[\"dW\" + str(L)] = dWL\n",
    "        gradients[\"db\" + str(L)] = dbL\n",
    "\n",
    "        \n",
    "        for l in reversed(range(1, L)):\n",
    "            cache_layer = cache[str(l)]\n",
    "            dZ = self.activationBackward(dA, cache_layer)\n",
    "            \n",
    "            # dropout\n",
    "            if self.drop_prob > 0:\n",
    "                dZ = self.dropout_backward(dZ, cache_layer)\n",
    "                \n",
    "            dA, dW, db = self.affineBackward(dZ, cache_layer)\n",
    "            \n",
    "            # L1/L2\n",
    "            if self.reg_lambda > 0 and reg_type is not None:\n",
    "                w_l= cache_layer[1]\n",
    "                if reg_type == \"L1\":\n",
    "                    dW += np.sign(w_l) * (self.reg_lambda / w_l.shape[1])\n",
    "                elif reg_type == \"L2\":\n",
    "                    dW += w_l * (self.reg_lambda / w_l.shape[1])\n",
    "            \n",
    "            gradients[\"dW\" + str(l)] = dW\n",
    "            gradients[\"db\" + str(l)] = db\n",
    "           \n",
    "        \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def updateParameters(self, gradients, alpha, reg_type=None):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param reg_type: regularization type. L1 or L2.\n",
    "        \"\"\"\n",
    "        delta=10e-6\n",
    "               \n",
    "        for l in range(1, self.num_layers):\n",
    "            \n",
    "            if self.momentum > 0 and self.decay_rate > 0:\n",
    "                self.parameters[\"vb\" + str(l)] = self.momentum*self.parameters[\"vb\" + str(l)] + (1-self.momentum)*gradients[\"db\" + str(l)]           \n",
    "                self.parameters[\"vW\" + str(l)] = self.momentum*self.parameters[\"vW\" + str(l)] + (1-self.momentum)*gradients[\"dW\" + str(l)]\n",
    "                \n",
    "                vb_prev = self.parameters[\"vb\" + str(l)]\n",
    "                sb_prev = self.parameters[\"sb\" + str(l)]\n",
    "                db_prev = gradients[\"db\" + str(l)]\n",
    "                self.parameters[\"sb\" + str(l)] = self.decay_rate*sb_prev + (1-self.decay_rate)*np.square(db_prev)\n",
    "                \n",
    "                vb_hat = self.parameters[\"vb\" + str(l)]/(1-self.momentum)\n",
    "                sb_hat = self.parameters[\"sb\" + str(l)]/(1-self.decay_rate)\n",
    "                self.parameters[\"b\" + str(l)] -= alpha*np.divide(vb_hat, np.sqrt(delta + sb_hat))\n",
    "                \n",
    "                vW_prev = self.parameters[\"vW\" + str(l)]\n",
    "                sW_prev = self.parameters[\"sW\" + str(l)]\n",
    "                dW_prev = gradients[\"dW\" + str(l)]\n",
    "                self.parameters[\"sW\" + str(l)] = self.decay_rate*sW_prev + (1-self.decay_rate)*np.square(dW_prev)\n",
    "                \n",
    "                vW_hat = self.parameters[\"vW\" + str(l)]/(1-self.momentum)\n",
    "                sW_hat = self.parameters[\"sW\" + str(l)]/(1-self.decay_rate)\n",
    "                self.parameters[\"W\" + str(l)] -= alpha*np.divide(vW_hat, np.sqrt(delta + sW_hat))\n",
    "                \n",
    "            elif self.momentum > 0:\n",
    "                self.parameters[\"vb\" + str(l)] = self.momentum*self.parameters[\"vb\" + str(l)] + (1-self.momentum)*gradients[\"db\" + str(l)]\n",
    "                self.parameters[\"b\" + str(l)] -= alpha*self.parameters[\"vb\" + str(l)]\n",
    "                \n",
    "                self.parameters[\"vW\" + str(l)] = self.momentum*self.parameters[\"vW\" + str(l)] + (1-self.momentum)*gradients[\"dW\" + str(l)]\n",
    "                self.parameters[\"W\" + str(l)] -= alpha*self.parameters[\"vW\" + str(l)]\n",
    "                \n",
    "            elif self.decay_rate > 0:\n",
    "                sb_prev = self.parameters[\"sb\" + str(l)]\n",
    "                db_prev = gradients[\"db\" + str(l)]\n",
    "                self.parameters[\"sb\" + str(l)] = self.decay_rate*sb_prev + (1-self.decay_rate)*np.square(db_prev)\n",
    "                self.parameters[\"b\" + str(l)] -= alpha*np.divide(db_prev, np.sqrt(delta + self.parameters[\"sb\" + str(l)]))\n",
    "                \n",
    "                sW_prev = self.parameters[\"sW\" + str(l)]\n",
    "                dW_prev = gradients[\"dW\" + str(l)]\n",
    "                self.parameters[\"sW\" + str(l)] = self.decay_rate*sW_prev + (1-self.decay_rate)*np.square(dW_prev)\n",
    "                self.parameters[\"W\" + str(l)] -= alpha*np.divide(dW_prev, np.sqrt(delta + self.parameters[\"sW\" + str(l)]))\n",
    "                \n",
    "            else:\n",
    "                self.parameters[\"b\" + str(l)] -= alpha*gradients[\"db\" + str(l)]\n",
    "                self.parameters[\"W\" + str(l)] -= alpha*gradients[\"dW\" + str(l)]\n",
    "\n",
    "                \n",
    "            # L1/L2\n",
    "            if self.reg_lambda > 0:\n",
    "                w_l = self.parameters[\"W\" + str(l)]\n",
    "                if reg_type == \"L1\":\n",
    "                    self.parameters[\"W\" + str(l)] -= alpha * np.sign(w_l) * (self.reg_lambda / w_l.shape[1])\n",
    "                elif reg_type == \"L2\":\n",
    "                    self.parameters[\"W\" + str(l)] -= alpha * w_l * (self.reg_lambda / w_l.shape[1])\n",
    "\n",
    "\n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100, \n",
    "              reg_type = None, CV=False, augment_data=False):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        :param reg_type: regularization type. L1 or L2.\n",
    "        \"\"\"\n",
    "        assert(y.shape[0]==X.shape[1])\n",
    "        \n",
    "        if augment_data==True:\n",
    "            X, y = self.augment_data(X, y)\n",
    "        \n",
    "        indices = np.random.permutation(y.shape[0])\n",
    "        cutoff = np.int(np.floor(0.9*y.shape[0]))\n",
    "        train_idx, val_idx = indices[:cutoff], indices[cutoff:]\n",
    "        \n",
    "        X_train = X[:, train_idx]\n",
    "        y_train = y[train_idx]\n",
    "        \n",
    "        X_val = X[:, val_idx]\n",
    "        y_val = y[val_idx]\n",
    "        \n",
    "        for i in range(0, iters):\n",
    "            # get minibatch\n",
    "            X_batch, y_batch = self.get_batch(X_train, y_train, batch_size)\n",
    "            \n",
    "            # forward prop\n",
    "            AL_batch, cache = self.forwardPropagation(X_batch)\n",
    "\n",
    "            # compute loss\n",
    "            cost_batch, dZL_batch = self.costFunction(AL_batch, y_batch, reg_type=reg_type)\n",
    "\n",
    "            # compute gradients\n",
    "            gradients_batch = self.backPropagation(dZL_batch, y_batch, cache, reg_type=reg_type)\n",
    "\n",
    "            # update weights and biases based on gradient\n",
    "            self.updateParameters(gradients_batch, alpha, reg_type=reg_type)\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                y_pred = self.predict(X_batch)\n",
    "                print(\"Training Batch Accuracy: \" + str(np.sum(y_pred==y_batch)/np.float(batch_size)))\n",
    "                print(\"Training Batch Cost: \" + str(cost_batch))\n",
    "                print(\"--------------------------\")\n",
    "                \n",
    "            \n",
    "        # forward prop\n",
    "        AL_val, cache_val = self.forwardPropagation(X_val)\n",
    "\n",
    "        # compute loss\n",
    "        cost_val, _ = self.costFunction(AL_val, y_val)\n",
    "        \n",
    "        y_pred_val = self.predict(X_val)\n",
    "        print(\"##########################\")\n",
    "        print(\"--------------------------\")\n",
    "        print(\"Validation Accuracy: \" + str(np.sum(y_pred_val==y_val)/np.float(y_val.shape[0])))\n",
    "        print(\"Validation Cost: \" + str(cost_val))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        AL, _ = self.forwardPropagation(X, training=False)\n",
    "        y_pred = np.argmax(AL, axis=0)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"  \n",
    "        idx = np.random.randint(y.size, size=batch_size)\n",
    "        X_batch = X[:, idx]\n",
    "        y_batch = y[idx]\n",
    "\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def augment_data(self, X, y):\n",
    "        \n",
    "        indices = np.random.permutation(y.shape[0])\n",
    "        cutoff = np.int(np.floor(0.2*y.shape[0]))\n",
    "        \n",
    "        X_shifted = X[:, indices[:cutoff]]\n",
    "        y_shifted = y[indices[:cutoff]]\n",
    "        \n",
    "        for col in range(X_shifted.shape[1]):\n",
    "            X_shifted[:, col] = shift(X_shifted[:, col], np.int(np.floor(X_shifted.shape[0])), mode='nearest')\n",
    "            \n",
    "        X_aug = np.hstack((X, X_shifted))\n",
    "        y_aug = np.concatenate((y, y_shifted))\n",
    "        \n",
    "        return X_aug, y_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((num_classes, y.shape[0]))\n",
    "    y_one_hot[y, range(y.shape[0])] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'horse': 7, 'automobile': 1, 'deer': 4, 'dog': 5, 'frog': 6, 'cat': 3, 'truck': 9, 'ship': 8, 'airplane': 0, 'bird': 2}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = '../cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "#### Simple fully-connected deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch Accuracy: 0.132\n",
      "Training Batch Cost: 2.30255618603\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.344\n",
      "Training Batch Cost: 1.87534155674\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.372\n",
      "Training Batch Cost: 1.72467492437\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.382\n",
      "Training Batch Cost: 1.65695733881\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.504\n",
      "Training Batch Cost: 1.60521459363\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.482\n",
      "Training Batch Cost: 1.49667822839\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.512\n",
      "Training Batch Cost: 1.37639570008\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.586\n",
      "Training Batch Cost: 1.27288725937\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.604\n",
      "Training Batch Cost: 1.252074392\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.612\n",
      "Training Batch Cost: 1.19341284537\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.644\n",
      "Training Batch Cost: 1.19372781157\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.638\n",
      "Training Batch Cost: 1.09634360157\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.668\n",
      "Training Batch Cost: 1.13370993563\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.658\n",
      "Training Batch Cost: 1.04102664494\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.726\n",
      "Training Batch Cost: 0.982042025679\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.716\n",
      "Training Batch Cost: 0.942704149488\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.752\n",
      "Training Batch Cost: 0.905417313331\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.748\n",
      "Training Batch Cost: 0.779389196163\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.77\n",
      "Training Batch Cost: 0.760718407771\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.796\n",
      "Training Batch Cost: 0.879070663654\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5114\n",
      "Validation Cost: 1.63282867667\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [X_train.shape[0], 500, 100, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=10000, alpha=0.0005, batch_size=500, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted = NN.predict(X_test)\n",
    "save_predictions('ans1-sy2569', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 8, 0, 4, 5, 8, 8, 2, 8, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-sy2569.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Improving the performance\n",
    "\n",
    "1. Regularizers: \n",
    "L1 regularization (Lasso), \n",
    "L2 regularization (Ridge), \n",
    "Dropout, \n",
    "Data Augmentation. \n",
    "\n",
    "2. Optimizers: \n",
    "SGD with momentum, \n",
    "rmsprop, \n",
    "adam.\n",
    "\n",
    "####  L1/L2 regularization\n",
    "* References: \n",
    "* L1: http://neuralnetworksanddeeplearning.com/chap3.html#other_techniques_for_regularization\n",
    "* L2: http://neuralnetworksanddeeplearning.com/chap3.html#regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions for trying different params\n",
    "def get_NNdict(drop_prob_list=[0], reg_lambda_list=[0], momentum_list = [0],decay_rate_list = [0]):\n",
    "    NN_dict = {}\n",
    "    for drop_prob in drop_prob_list:\n",
    "        for reg_lambda in reg_lambda_list:\n",
    "            for momentum in momentum_list:\n",
    "                for decay_rate in decay_rate_list:\n",
    "                    NN_val = NeuralNetwork(layer_dimensions, drop_prob=drop_prob, reg_lambda=reg_lambda, \n",
    "                                           momentum=momentum,decay_rate=decay_rate)\n",
    "                    NN_dict[(drop_prob, reg_lambda, momentum, decay_rate)] = NN_val\n",
    "    return NN_dict\n",
    "\n",
    "\n",
    "def train_multipleNN(NN_dict, iters_list=[5000], alpha_list=[0.0005], \n",
    "                     batch_size_list =[500], reg_type_list=[None], augment_data_list=[False]):\n",
    "    for iters in iters_list:\n",
    "        for alpha in alpha_list:\n",
    "            for batch_size in batch_size_list:\n",
    "                for augment_data in augment_data_list: \n",
    "                    for param, nn in NN_dict.iteritems():\n",
    "                        print \"#####################################\"\n",
    "                        print \"iters:\", iters\n",
    "                        print \"alpha: \", alpha\n",
    "                        print \"batch_size: \", batch_size\n",
    "                        print \"augment_data: \", augment_data\n",
    "                        print \"drop_prob, reg_lambda, momentum, decay_rate: \", param\n",
    "\n",
    "                        if param[1] > 0: # if reg_lambda > 0\n",
    "                            for reg_type in reg_type_list:\n",
    "                                print \"reg_type:\", reg_type\n",
    "                                print \"#####################################\"\n",
    "                                nn.train(X_train, y_train, iters=iters, alpha=alpha, \n",
    "                                     batch_size=batch_size, print_every=500, reg_type=reg_type,\n",
    "                                         augment_data=augment_data)\n",
    "                        else: # if reg_lambda == 0\n",
    "                            print \"#####################################\"\n",
    "                            nn.train(X_train, y_train, iters=iters, alpha=alpha, \n",
    "                                     batch_size=batch_size, print_every=500, augment_data=augment_data)\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.3, 0, 0)\n",
      "reg_type: L1\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.132\n",
      "Training Batch Cost: 2.49309865705\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.328\n",
      "Training Batch Cost: 2.55485201363\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.356\n",
      "Training Batch Cost: 2.78796608053\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.418\n",
      "Training Batch Cost: 2.88013677916\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.496\n",
      "Training Batch Cost: 3.15976799618\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.452\n",
      "Training Batch Cost: 3.25829872552\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.524\n",
      "Training Batch Cost: 3.43365580609\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.588\n",
      "Training Batch Cost: 3.57275107328\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.622\n",
      "Training Batch Cost: 3.71589359131\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.656\n",
      "Training Batch Cost: 3.85883005714\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5002\n",
      "Validation Cost: 1.44203273967\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.2, 0, 0)\n",
      "reg_type: L1\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.19\n",
      "Training Batch Cost: 2.42961117961\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.328\n",
      "Training Batch Cost: 2.26195948383\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.346\n",
      "Training Batch Cost: 2.44213676503\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.46\n",
      "Training Batch Cost: 2.38598875274\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.498\n",
      "Training Batch Cost: 2.46275915919\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.514\n",
      "Training Batch Cost: 2.54421773412\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.554\n",
      "Training Batch Cost: 2.64860016344\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.56\n",
      "Training Batch Cost: 2.8928800103\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.584\n",
      "Training Batch Cost: 2.83429574509\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.572\n",
      "Training Batch Cost: 3.05496573959\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5102\n",
      "Validation Cost: 1.41025974169\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.05, 0, 0)\n",
      "reg_type: L1\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.11\n",
      "Training Batch Cost: 2.33433808621\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.34\n",
      "Training Batch Cost: 1.98963740677\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.42\n",
      "Training Batch Cost: 1.87091457917\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.41\n",
      "Training Batch Cost: 1.85858773141\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.462\n",
      "Training Batch Cost: 1.85322911669\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.526\n",
      "Training Batch Cost: 1.70807948518\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.544\n",
      "Training Batch Cost: 1.76274504766\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.524\n",
      "Training Batch Cost: 1.77635313937\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.63\n",
      "Training Batch Cost: 1.60311952626\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.632\n",
      "Training Batch Cost: 1.59523651966\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.4922\n",
      "Validation Cost: 1.39819032245\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.1, 0, 0)\n",
      "reg_type: L1\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.112\n",
      "Training Batch Cost: 2.36609495416\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.308\n",
      "Training Batch Cost: 2.16265387554\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.372\n",
      "Training Batch Cost: 2.06527019619\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.454\n",
      "Training Batch Cost: 1.99420619059\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.466\n",
      "Training Batch Cost: 2.04617792232\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.424\n",
      "Training Batch Cost: 2.15037208495\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.574\n",
      "Training Batch Cost: 2.04977824271\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.562\n",
      "Training Batch Cost: 2.17016786335\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.61\n",
      "Training Batch Cost: 2.03032636617\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.612\n",
      "Training Batch Cost: 2.08993694284\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.4796\n",
      "Validation Cost: 1.49664127544\n"
     ]
    }
   ],
   "source": [
    "### L1 Only\n",
    "reg_lambda_list = [0.05,0.1,0.2,0.3]\n",
    "\n",
    "NN_dict = get_NNdict(reg_lambda_list=reg_lambda_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "reg_type_list=[\"L1\"]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                     batch_size_list =batch_size_list, reg_type_list=reg_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 10000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.2, 0, 0)\n",
      "reg_type: L1\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.132\n",
      "Training Batch Cost: 2.42958450004\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.33\n",
      "Training Batch Cost: 2.39995950189\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.374\n",
      "Training Batch Cost: 2.46852385268\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.416\n",
      "Training Batch Cost: 2.488543511\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.506\n",
      "Training Batch Cost: 2.62846418549\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.474\n",
      "Training Batch Cost: 2.67475789706\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.536\n",
      "Training Batch Cost: 2.72428243987\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.6\n",
      "Training Batch Cost: 2.77285690356\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.59\n",
      "Training Batch Cost: 2.87794892316\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.65\n",
      "Training Batch Cost: 2.95510753399\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.618\n",
      "Training Batch Cost: 3.12915703307\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.648\n",
      "Training Batch Cost: 3.18170235821\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.658\n",
      "Training Batch Cost: 3.52879945322\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.68\n",
      "Training Batch Cost: 3.48734311383\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.732\n",
      "Training Batch Cost: 3.57784055314\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.668\n",
      "Training Batch Cost: 3.77478254213\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.726\n",
      "Training Batch Cost: 3.83537897768\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.742\n",
      "Training Batch Cost: 3.9531866182\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.79\n",
      "Training Batch Cost: 3.97487487583\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.776\n",
      "Training Batch Cost: 4.19519028358\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5156\n",
      "Validation Cost: 1.59272382519\n"
     ]
    }
   ],
   "source": [
    "### L1 Only with the best param 0.2 with 10,000 iterations\n",
    "reg_lambda_list = [0.2]\n",
    "\n",
    "NN_dict = get_NNdict(reg_lambda_list=reg_lambda_list)\n",
    "\n",
    "iters_list=[10000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "reg_type_list=[\"L1\"]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                     batch_size_list =batch_size_list, reg_type_list=reg_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.5, 0, 0)\n",
      "reg_type: L2\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.132\n",
      "Training Batch Cost: 2.30372470854\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.342\n",
      "Training Batch Cost: 1.93787594364\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.382\n",
      "Training Batch Cost: 1.78883221319\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.392\n",
      "Training Batch Cost: 1.6735282525\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.498\n",
      "Training Batch Cost: 1.644808124\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.492\n",
      "Training Batch Cost: 1.5967884718\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.558\n",
      "Training Batch Cost: 1.49013555679\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.578\n",
      "Training Batch Cost: 1.440475051\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.584\n",
      "Training Batch Cost: 1.41703834187\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.644\n",
      "Training Batch Cost: 1.36928638471\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5198\n",
      "Validation Cost: 1.36942454319\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.1, 0, 0)\n",
      "reg_type: L2\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.19\n",
      "Training Batch Cost: 2.30281657009\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.324\n",
      "Training Batch Cost: 1.82876087041\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.346\n",
      "Training Batch Cost: 1.84572644619\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.426\n",
      "Training Batch Cost: 1.54391383188\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.522\n",
      "Training Batch Cost: 1.49139673043\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.506\n",
      "Training Batch Cost: 1.42509595061\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.494\n",
      "Training Batch Cost: 1.38413006774\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.556\n",
      "Training Batch Cost: 1.36344179004\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.584\n",
      "Training Batch Cost: 1.26020262159\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.58\n",
      "Training Batch Cost: 1.32295239291\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5082\n",
      "Validation Cost: 1.40016884571\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.2, 0, 0)\n",
      "reg_type: L2\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.11\n",
      "Training Batch Cost: 2.30304841671\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.358\n",
      "Training Batch Cost: 1.8773260955\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.398\n",
      "Training Batch Cost: 1.68344119059\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.44\n",
      "Training Batch Cost: 1.69136606755\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.398\n",
      "Training Batch Cost: 1.7011934477\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.512\n",
      "Training Batch Cost: 1.44211539849\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.506\n",
      "Training Batch Cost: 1.50161813295\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.58\n",
      "Training Batch Cost: 1.34598768725\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.612\n",
      "Training Batch Cost: 1.31687085614\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.594\n",
      "Training Batch Cost: 1.32622472275\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.486\n",
      "Validation Cost: 1.4479744261\n"
     ]
    }
   ],
   "source": [
    "### L2 Only\n",
    "reg_lambda_list = [0.1,0.2,0.5]\n",
    "\n",
    "NN_dict = get_NNdict(reg_lambda_list=reg_lambda_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "reg_type_list=[\"L2\"]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                     batch_size_list =batch_size_list, reg_type_list=reg_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0.1, 0, 0, 0)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.132\n",
      "Training Batch Cost: 2.30255668218\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.324\n",
      "Training Batch Cost: 1.85582917039\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.39\n",
      "Training Batch Cost: 1.71304807972\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.45\n",
      "Training Batch Cost: 1.57018949093\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.488\n",
      "Training Batch Cost: 1.54944068152\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.496\n",
      "Training Batch Cost: 1.46955981939\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.514\n",
      "Training Batch Cost: 1.4449458248\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.554\n",
      "Training Batch Cost: 1.39847885148\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.558\n",
      "Training Batch Cost: 1.45919210993\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.596\n",
      "Training Batch Cost: 1.32514922582\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5076\n",
      "Validation Cost: 1.42005658296\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0.3, 0, 0, 0)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.114\n",
      "Training Batch Cost: 2.30258647259\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.246\n",
      "Training Batch Cost: 1.91159518213\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.37\n",
      "Training Batch Cost: 1.71483130316\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.418\n",
      "Training Batch Cost: 1.64575194324\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.486\n",
      "Training Batch Cost: 1.60707767881\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.458\n",
      "Training Batch Cost: 1.61325775675\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.486\n",
      "Training Batch Cost: 1.66909083363\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.532\n",
      "Training Batch Cost: 1.44912799504\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.514\n",
      "Training Batch Cost: 1.45323178427\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.554\n",
      "Training Batch Cost: 1.38209726929\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.514\n",
      "Validation Cost: 1.47577780298\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0.2, 0, 0, 0)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.128\n",
      "Training Batch Cost: 2.30257587382\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.292\n",
      "Training Batch Cost: 1.95274560609\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.414\n",
      "Training Batch Cost: 1.71164841938\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.422\n",
      "Training Batch Cost: 1.64483916272\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.426\n",
      "Training Batch Cost: 1.73784814716\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.552\n",
      "Training Batch Cost: 1.50240924793\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.504\n",
      "Training Batch Cost: 1.62792608101\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.56\n",
      "Training Batch Cost: 1.41084099073\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.552\n",
      "Training Batch Cost: 1.33491247016\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.578\n",
      "Training Batch Cost: 1.39327770529\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5172\n",
      "Validation Cost: 1.46759100271\n"
     ]
    }
   ],
   "source": [
    "### Dropout Only\n",
    "drop_prob_list = [0.1,0.2,0.3]\n",
    "\n",
    "NN_dict = get_NNdict(drop_prob_list=drop_prob_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, batch_size_list =batch_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  True\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0, 0, 0)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.122\n",
      "Training Batch Cost: 2.30258520503\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.296\n",
      "Training Batch Cost: 1.93273325836\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.344\n",
      "Training Batch Cost: 1.84657892951\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.368\n",
      "Training Batch Cost: 1.79857179604\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.38\n",
      "Training Batch Cost: 1.79746819308\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.44\n",
      "Training Batch Cost: 1.64388228906\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.436\n",
      "Training Batch Cost: 1.65070178642\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.422\n",
      "Training Batch Cost: 1.69012723863\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.522\n",
      "Training Batch Cost: 1.47323137836\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.508\n",
      "Training Batch Cost: 1.55494925556\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.402333333333\n",
      "Validation Cost: 1.68117675106\n"
     ]
    }
   ],
   "source": [
    "### Data Aug Only\n",
    "NN_dict = get_NNdict()\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "augment_data_list=[True]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                 batch_size_list =batch_size_list, augment_data_list=augment_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0, 0.9, 0)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.132\n",
      "Training Batch Cost: 2.30255618603\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.356\n",
      "Training Batch Cost: 1.75162571114\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.472\n",
      "Training Batch Cost: 1.55207203588\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.546\n",
      "Training Batch Cost: 1.4265186387\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.534\n",
      "Training Batch Cost: 1.31766765481\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.572\n",
      "Training Batch Cost: 1.25365865113\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.598\n",
      "Training Batch Cost: 1.15116804342\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.632\n",
      "Training Batch Cost: 1.03822685205\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.658\n",
      "Training Batch Cost: 1.01430964353\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.712\n",
      "Training Batch Cost: 0.879469139309\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.549\n",
      "Validation Cost: 1.37342113364\n"
     ]
    }
   ],
   "source": [
    "### Momentum Only\n",
    "momentum_list = [0.9]\n",
    "\n",
    "NN_dict = get_NNdict(momentum_list = momentum_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, batch_size_list =batch_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0, 0, 0.9)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.164\n",
      "Training Batch Cost: 2.30255618603\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.372\n",
      "Training Batch Cost: 1.79618202295\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.4\n",
      "Training Batch Cost: 1.67347136983\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.478\n",
      "Training Batch Cost: 1.62311930166\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.506\n",
      "Training Batch Cost: 1.4626379222\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.488\n",
      "Training Batch Cost: 1.53218247658\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.556\n",
      "Training Batch Cost: 1.33299735767\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.59\n",
      "Training Batch Cost: 1.27841202331\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.604\n",
      "Training Batch Cost: 1.2895410981\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.638\n",
      "Training Batch Cost: 1.11756915563\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5026\n",
      "Validation Cost: 1.42874056096\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0, 0, 0.97)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.12\n",
      "Training Batch Cost: 2.30258286559\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.36\n",
      "Training Batch Cost: 1.74793710903\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.446\n",
      "Training Batch Cost: 1.65317897528\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.47\n",
      "Training Batch Cost: 1.53171963013\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.518\n",
      "Training Batch Cost: 1.42776939957\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.528\n",
      "Training Batch Cost: 1.36609544593\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.538\n",
      "Training Batch Cost: 1.35839939152\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.564\n",
      "Training Batch Cost: 1.39967598696\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.522\n",
      "Training Batch Cost: 1.32842212823\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.568\n",
      "Training Batch Cost: 1.27466436768\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.492\n",
      "Validation Cost: 1.42813526564\n"
     ]
    }
   ],
   "source": [
    "### RMSprop Only\n",
    "decay_rate_list = [0.9, 0.97]\n",
    "\n",
    "NN_dict = get_NNdict(decay_rate_list = decay_rate_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, batch_size_list =batch_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0, 0.9, 0.97)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.19\n",
      "Training Batch Cost: 2.30255618603\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.422\n",
      "Training Batch Cost: 1.59536979082\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.496\n",
      "Training Batch Cost: 1.45233579815\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.582\n",
      "Training Batch Cost: 1.31261943578\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.578\n",
      "Training Batch Cost: 1.23480210994\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.594\n",
      "Training Batch Cost: 1.22335926902\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.6\n",
      "Training Batch Cost: 1.11990540935\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.63\n",
      "Training Batch Cost: 1.05839372219\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.674\n",
      "Training Batch Cost: 0.988834719043\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.714\n",
      "Training Batch Cost: 0.875626641816\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5392\n",
      "Validation Cost: 1.43265178636\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0, 0.9, 0.99)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.12\n",
      "Training Batch Cost: 2.30258286559\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.406\n",
      "Training Batch Cost: 1.62165160224\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.45\n",
      "Training Batch Cost: 1.56341514051\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.496\n",
      "Training Batch Cost: 1.39230326695\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.562\n",
      "Training Batch Cost: 1.30089736599\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.516\n",
      "Training Batch Cost: 1.29165888466\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.586\n",
      "Training Batch Cost: 1.16013576317\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.62\n",
      "Training Batch Cost: 1.17072064129\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.6\n",
      "Training Batch Cost: 1.09910163026\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.6\n",
      "Training Batch Cost: 1.12806990082\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5302\n",
      "Validation Cost: 1.3625191186\n"
     ]
    }
   ],
   "source": [
    "### Adam Only\n",
    "momentum_list = [0.9]\n",
    "decay_rate_list = [0.97,0.99]\n",
    "\n",
    "NN_dict = get_NNdict(momentum_list=momentum_list, decay_rate_list = decay_rate_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, batch_size_list =batch_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.2, 0.9, 0)\n",
      "reg_type: L1\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.132\n",
      "Training Batch Cost: 2.42958450004\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.366\n",
      "Training Batch Cost: 2.18580912398\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.466\n",
      "Training Batch Cost: 2.26255491041\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.55\n",
      "Training Batch Cost: 2.25179259149\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.548\n",
      "Training Batch Cost: 2.40824584826\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.556\n",
      "Training Batch Cost: 2.53545772043\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.646\n",
      "Training Batch Cost: 2.5187530317\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.638\n",
      "Training Batch Cost: 2.60625452664\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.682\n",
      "Training Batch Cost: 2.72753416719\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.674\n",
      "Training Batch Cost: 2.82456547228\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5362\n",
      "Validation Cost: 1.423388435\n"
     ]
    }
   ],
   "source": [
    "### Momentum with L1\n",
    "reg_lambda_list = [0.2]\n",
    "momentum_list = [0.9]\n",
    "\n",
    "NN_dict = get_NNdict(reg_lambda_list=reg_lambda_list, momentum_list = momentum_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "reg_type_list=[\"L1\"]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                 batch_size_list =batch_size_list, reg_type_list=reg_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.1, 0.9, 0)\n",
      "reg_type: L2\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.132\n",
      "Training Batch Cost: 2.30278989053\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.36\n",
      "Training Batch Cost: 1.75324544519\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.462\n",
      "Training Batch Cost: 1.58047768398\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.564\n",
      "Training Batch Cost: 1.41012966936\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.562\n",
      "Training Batch Cost: 1.35197887256\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.59\n",
      "Training Batch Cost: 1.2792670906\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.612\n",
      "Training Batch Cost: 1.17207507779\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.648\n",
      "Training Batch Cost: 1.0806613816\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.664\n",
      "Training Batch Cost: 1.02320867627\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.698\n",
      "Training Batch Cost: 1.00386141328\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5434\n",
      "Validation Cost: 1.38104840573\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.5, 0.9, 0)\n",
      "reg_type: L2\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.12\n",
      "Training Batch Cost: 2.30375138811\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.36\n",
      "Training Batch Cost: 1.71188196604\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.43\n",
      "Training Batch Cost: 1.65036163068\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.526\n",
      "Training Batch Cost: 1.42148884341\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.55\n",
      "Training Batch Cost: 1.37958397613\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.568\n",
      "Training Batch Cost: 1.33951634369\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.612\n",
      "Training Batch Cost: 1.27428062474\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.632\n",
      "Training Batch Cost: 1.22078968844\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.634\n",
      "Training Batch Cost: 1.18943568053\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.652\n",
      "Training Batch Cost: 1.29412685373\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5284\n",
      "Validation Cost: 1.41194990753\n"
     ]
    }
   ],
   "source": [
    "### Momentum with L2\n",
    "reg_lambda_list = [0.5,0.1]\n",
    "momentum_list = [0.9]\n",
    "\n",
    "NN_dict = get_NNdict(reg_lambda_list=reg_lambda_list, momentum_list = momentum_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "reg_type_list=[\"L2\"]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                 batch_size_list =batch_size_list, reg_type_list=reg_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0.2, 0, 0.9, 0)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.132\n",
      "Training Batch Cost: 2.3025557013\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.358\n",
      "Training Batch Cost: 1.78465714735\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.472\n",
      "Training Batch Cost: 1.60039898465\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.526\n",
      "Training Batch Cost: 1.51765677675\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.518\n",
      "Training Batch Cost: 1.38501915797\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.512\n",
      "Training Batch Cost: 1.44631166844\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.558\n",
      "Training Batch Cost: 1.32753060897\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.528\n",
      "Training Batch Cost: 1.36172232124\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.596\n",
      "Training Batch Cost: 1.33073741454\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.596\n",
      "Training Batch Cost: 1.27257702808\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5288\n",
      "Validation Cost: 1.42189426078\n"
     ]
    }
   ],
   "source": [
    "### Momentum with Dropout\n",
    "drop_prob_list = [0.2] # pick the best one from L1 only\n",
    "momentum_list = [0.9] # pick the best one from momentum only\n",
    "\n",
    "NN_dict = get_NNdict(drop_prob_list=drop_prob_list, momentum_list = momentum_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, batch_size_list =batch_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  True\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0, 0.9, 0)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.09\n",
      "Training Batch Cost: 2.30258520503\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.338\n",
      "Training Batch Cost: 1.8671357464\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.346\n",
      "Training Batch Cost: 1.73702742171\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.436\n",
      "Training Batch Cost: 1.56826451832\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.426\n",
      "Training Batch Cost: 1.50245062804\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.466\n",
      "Training Batch Cost: 1.50120668859\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.494\n",
      "Training Batch Cost: 1.39433884727\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.49\n",
      "Training Batch Cost: 1.397273094\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.56\n",
      "Training Batch Cost: 1.31074299427\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.532\n",
      "Training Batch Cost: 1.31480025568\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.455\n",
      "Validation Cost: 1.59698555794\n"
     ]
    }
   ],
   "source": [
    "### Momentum with Data Aug\n",
    "momentum_list = [0.9]\n",
    "\n",
    "NN_dict = get_NNdict(momentum_list=momentum_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "augment_data_list=[True]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                 batch_size_list =batch_size_list, augment_data_list=augment_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.2, 0, 0.9)\n",
      "reg_type: L1\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.16\n",
      "Training Batch Cost: 2.42958450004\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.352\n",
      "Training Batch Cost: 2.32767468076\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.424\n",
      "Training Batch Cost: 2.34079223735\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.474\n",
      "Training Batch Cost: 2.39759447126\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.486\n",
      "Training Batch Cost: 2.41201669912\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.49\n",
      "Training Batch Cost: 2.55599219774\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.55\n",
      "Training Batch Cost: 2.49780739427\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.54\n",
      "Training Batch Cost: 2.51133769811\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.598\n",
      "Training Batch Cost: 2.5260478016\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.594\n",
      "Training Batch Cost: 2.55102940986\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5056\n",
      "Validation Cost: 1.40742368539\n"
     ]
    }
   ],
   "source": [
    "### RMSprop with L1\n",
    "reg_lambda_list = [0.2]\n",
    "decay_rate_list = [0.9]\n",
    "\n",
    "NN_dict = get_NNdict(reg_lambda_list = reg_lambda_list, decay_rate_list = decay_rate_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "reg_type_list=[\"L1\"]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                 batch_size_list =batch_size_list, reg_type_list=reg_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.1, 0, 0.9)\n",
      "reg_type: L2\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.164\n",
      "Training Batch Cost: 2.30278989053\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.364\n",
      "Training Batch Cost: 1.81893833758\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.412\n",
      "Training Batch Cost: 1.74767086268\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.484\n",
      "Training Batch Cost: 1.65260778591\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.522\n",
      "Training Batch Cost: 1.4760423858\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.508\n",
      "Training Batch Cost: 1.46279430898\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.542\n",
      "Training Batch Cost: 1.37523610887\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.552\n",
      "Training Batch Cost: 1.31100921869\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.632\n",
      "Training Batch Cost: 1.22009599559\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.596\n",
      "Training Batch Cost: 1.19698716117\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5104\n",
      "Validation Cost: 1.39857228865\n"
     ]
    }
   ],
   "source": [
    "### RMSprop with L2\n",
    "reg_lambda_list = [0.1]\n",
    "decay_rate_list = [0.9]\n",
    "\n",
    "NN_dict = get_NNdict(reg_lambda_list = reg_lambda_list, decay_rate_list = decay_rate_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "reg_type_list=[\"L2\"]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                 batch_size_list =batch_size_list, reg_type_list=reg_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0.2, 0, 0, 0.9)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.158\n",
      "Training Batch Cost: 2.3025557013\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.366\n",
      "Training Batch Cost: 1.8005487181\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.418\n",
      "Training Batch Cost: 1.69096379485\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.472\n",
      "Training Batch Cost: 1.60162678262\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.516\n",
      "Training Batch Cost: 1.49848005299\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.484\n",
      "Training Batch Cost: 1.53076299613\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.54\n",
      "Training Batch Cost: 1.42452757653\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.544\n",
      "Training Batch Cost: 1.42773541393\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.566\n",
      "Training Batch Cost: 1.43596573957\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.566\n",
      "Training Batch Cost: 1.35771981281\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5018\n",
      "Validation Cost: 1.4611017253\n"
     ]
    }
   ],
   "source": [
    "### RMSprop with Dropout\n",
    "drop_prob_list = [0.2]\n",
    "decay_rate_list = [0.9]\n",
    "\n",
    "NN_dict = get_NNdict(drop_prob_list = drop_prob_list, decay_rate_list = decay_rate_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, batch_size_list =batch_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  True\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0, 0, 0.9)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.15\n",
      "Training Batch Cost: 2.30258520503\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.354\n",
      "Training Batch Cost: 1.87295706897\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.36\n",
      "Training Batch Cost: 1.80337542238\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.408\n",
      "Training Batch Cost: 1.67272401492\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.418\n",
      "Training Batch Cost: 1.63479622153\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.45\n",
      "Training Batch Cost: 1.60485765873\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.464\n",
      "Training Batch Cost: 1.47957938026\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.398\n",
      "Training Batch Cost: 1.66952751647\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.534\n",
      "Training Batch Cost: 1.39071832808\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.522\n",
      "Training Batch Cost: 1.43155579638\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.436666666667\n",
      "Validation Cost: 1.57371475238\n"
     ]
    }
   ],
   "source": [
    "### RMSprop with Data Aug\n",
    "decay_rate_list = [0.9]\n",
    "\n",
    "NN_dict = get_NNdict(decay_rate_list = decay_rate_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "augment_data_list=[True]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                 batch_size_list =batch_size_list, augment_data_list=augment_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.2, 0.9, 0.9)\n",
      "reg_type: L1\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.188\n",
      "Training Batch Cost: 2.42958450004\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.444\n",
      "Training Batch Cost: 2.72023132812\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.54\n",
      "Training Batch Cost: 2.68122447556\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.62\n",
      "Training Batch Cost: 2.68995265733\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.6\n",
      "Training Batch Cost: 2.80159431802\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.62\n",
      "Training Batch Cost: 2.87933071888\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.656\n",
      "Training Batch Cost: 3.0279917995\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.648\n",
      "Training Batch Cost: 3.0507189975\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.694\n",
      "Training Batch Cost: 3.17870525745\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.71\n",
      "Training Batch Cost: 3.25130309315\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5284\n",
      "Validation Cost: 1.54420343269\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.2, 0.9, 0.95)\n",
      "reg_type: L1\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.12\n",
      "Training Batch Cost: 2.42961117961\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.458\n",
      "Training Batch Cost: 2.572567915\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.49\n",
      "Training Batch Cost: 2.70062644483\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.56\n",
      "Training Batch Cost: 2.64504928049\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.578\n",
      "Training Batch Cost: 2.72431249884\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.604\n",
      "Training Batch Cost: 2.87118710371\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.62\n",
      "Training Batch Cost: 2.90445315437\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.69\n",
      "Training Batch Cost: 2.93054615242\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.648\n",
      "Training Batch Cost: 3.08959398614\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.69\n",
      "Training Batch Cost: 3.22818841983\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5354\n",
      "Validation Cost: 1.44049295814\n"
     ]
    }
   ],
   "source": [
    "### Adam with L1\n",
    "reg_lambda_list = [0.2]\n",
    "momentum_list = [0.9]\n",
    "decay_rate_list = [0.9, 0.95]\n",
    "\n",
    "NN_dict = get_NNdict(reg_lambda_list = reg_lambda_list, momentum_list=momentum_list, \n",
    "                     decay_rate_list = decay_rate_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "reg_type_list=[\"L1\"]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                 batch_size_list =batch_size_list, reg_type_list=reg_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.2, 0.9, 0.97)\n",
      "reg_type: L1\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.188\n",
      "Training Batch Cost: 2.42958450004\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.44\n",
      "Training Batch Cost: 2.54324081285\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.496\n",
      "Training Batch Cost: 2.54858897593\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.596\n",
      "Training Batch Cost: 2.52944759766\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.572\n",
      "Training Batch Cost: 2.65190908441\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.6\n",
      "Training Batch Cost: 2.73903002166\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.602\n",
      "Training Batch Cost: 2.8746374933\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.64\n",
      "Training Batch Cost: 2.86144533351\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.69\n",
      "Training Batch Cost: 2.96919615016\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.696\n",
      "Training Batch Cost: 2.96810679099\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5402\n",
      "Validation Cost: 1.41820539924\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.2, 0.9, 0.98)\n",
      "reg_type: L1\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.12\n",
      "Training Batch Cost: 2.42961117961\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.42\n",
      "Training Batch Cost: 2.46009516221\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.486\n",
      "Training Batch Cost: 2.55981563167\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.514\n",
      "Training Batch Cost: 2.51097977693\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.574\n",
      "Training Batch Cost: 2.59305279353\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.538\n",
      "Training Batch Cost: 2.66665789401\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.618\n",
      "Training Batch Cost: 2.68445545752\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.666\n",
      "Training Batch Cost: 2.73394983203\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.634\n",
      "Training Batch Cost: 2.84911198828\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.648\n",
      "Training Batch Cost: 3.01022887064\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5382\n",
      "Validation Cost: 1.38066137322\n"
     ]
    }
   ],
   "source": [
    "### Adam with L1 - more combinations\n",
    "reg_lambda_list = [0.2]\n",
    "momentum_list = [0.9]\n",
    "decay_rate_list = [0.97, 0.98]\n",
    "\n",
    "NN_dict = get_NNdict(reg_lambda_list = reg_lambda_list, momentum_list=momentum_list, \n",
    "                     decay_rate_list = decay_rate_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "reg_type_list=[\"L1\"]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                 batch_size_list =batch_size_list, reg_type_list=reg_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.1, 0.9, 0.97)\n",
      "reg_type: L2\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.19\n",
      "Training Batch Cost: 2.30278989053\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.424\n",
      "Training Batch Cost: 1.61421651017\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.5\n",
      "Training Batch Cost: 1.47612278069\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.566\n",
      "Training Batch Cost: 1.33036313326\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.562\n",
      "Training Batch Cost: 1.29687357607\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.564\n",
      "Training Batch Cost: 1.2457862823\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.6\n",
      "Training Batch Cost: 1.19125179438\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.658\n",
      "Training Batch Cost: 1.08215806221\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.656\n",
      "Training Batch Cost: 1.06609491996\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.72\n",
      "Training Batch Cost: 0.916491221956\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5332\n",
      "Validation Cost: 1.41617966321\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0.1, 0.9, 0.9)\n",
      "reg_type: L2\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.12\n",
      "Training Batch Cost: 2.30281657009\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.47\n",
      "Training Batch Cost: 1.52071255749\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.51\n",
      "Training Batch Cost: 1.41996049785\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.55\n",
      "Training Batch Cost: 1.28784843423\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.574\n",
      "Training Batch Cost: 1.25057465213\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.598\n",
      "Training Batch Cost: 1.15476635494\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.632\n",
      "Training Batch Cost: 1.11860685057\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.676\n",
      "Training Batch Cost: 1.07146296732\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.664\n",
      "Training Batch Cost: 1.06400552875\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.688\n",
      "Training Batch Cost: 1.04808808496\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.517\n",
      "Validation Cost: 1.62336260731\n"
     ]
    }
   ],
   "source": [
    "### Adam with L2\n",
    "reg_lambda_list = [0.1]\n",
    "momentum_list = [0.9]\n",
    "decay_rate_list = [0.9,0.97]\n",
    "\n",
    "NN_dict = get_NNdict(reg_lambda_list = reg_lambda_list, momentum_list=momentum_list, \n",
    "                     decay_rate_list = decay_rate_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "reg_type_list=[\"L2\"]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                 batch_size_list =batch_size_list, reg_type_list=reg_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0.2, 0, 0.9, 0.97)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.186\n",
      "Training Batch Cost: 2.3025557013\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.398\n",
      "Training Batch Cost: 1.70482198051\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.518\n",
      "Training Batch Cost: 1.45866627706\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.544\n",
      "Training Batch Cost: 1.44526188238\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.546\n",
      "Training Batch Cost: 1.3310626344\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.566\n",
      "Training Batch Cost: 1.40267300701\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.582\n",
      "Training Batch Cost: 1.33552731451\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.566\n",
      "Training Batch Cost: 1.3111122936\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.62\n",
      "Training Batch Cost: 1.25392215271\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.616\n",
      "Training Batch Cost: 1.23852719543\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5316\n",
      "Validation Cost: 1.45757939295\n",
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  False\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0.2, 0, 0.9, 0.9)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.114\n",
      "Training Batch Cost: 2.30258637916\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.438\n",
      "Training Batch Cost: 1.58627561111\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.504\n",
      "Training Batch Cost: 1.5145295926\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.528\n",
      "Training Batch Cost: 1.4716024313\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.516\n",
      "Training Batch Cost: 1.46634980786\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.53\n",
      "Training Batch Cost: 1.40880056989\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.558\n",
      "Training Batch Cost: 1.37264884342\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.578\n",
      "Training Batch Cost: 1.27930265762\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.582\n",
      "Training Batch Cost: 1.30899241264\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.584\n",
      "Training Batch Cost: 1.26346407464\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5154\n",
      "Validation Cost: 1.51600790111\n"
     ]
    }
   ],
   "source": [
    "### Adam with Dropout\n",
    "drop_prob_list = [0.2]\n",
    "momentum_list = [0.9]\n",
    "decay_rate_list = [0.9, 0.97]\n",
    "\n",
    "NN_dict = get_NNdict(drop_prob_list = drop_prob_list, momentum_list=momentum_list, \n",
    "                     decay_rate_list = decay_rate_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, batch_size_list =batch_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "iters: 5000\n",
      "alpha:  0.0005\n",
      "batch_size:  500\n",
      "augment_data:  True\n",
      "drop_prob, reg_lambda, momentum, decay_rate:  (0, 0, 0.9, 0.97)\n",
      "#####################################\n",
      "Training Batch Accuracy: 0.122\n",
      "Training Batch Cost: 2.30258520503\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.37\n",
      "Training Batch Cost: 1.76238859811\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.376\n",
      "Training Batch Cost: 1.68021782792\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.428\n",
      "Training Batch Cost: 1.5818893374\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.446\n",
      "Training Batch Cost: 1.51982548963\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.454\n",
      "Training Batch Cost: 1.45919978936\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.512\n",
      "Training Batch Cost: 1.40721908807\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.48\n",
      "Training Batch Cost: 1.4150400162\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.552\n",
      "Training Batch Cost: 1.28225890658\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.532\n",
      "Training Batch Cost: 1.32670777807\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.463666666667\n",
      "Validation Cost: 1.54095765855\n"
     ]
    }
   ],
   "source": [
    "### Adam with Data Aug\n",
    "momentum_list = [0.9]\n",
    "decay_rate_list = [0.97]\n",
    "\n",
    "NN_dict = get_NNdict(momentum_list=momentum_list, decay_rate_list = decay_rate_list)\n",
    "\n",
    "iters_list=[5000]\n",
    "alpha_list=[0.0005] \n",
    "batch_size_list =[500]\n",
    "augment_data_list=[True]\n",
    "\n",
    "train_multipleNN(NN_dict,iters_list=iters_list, alpha_list=alpha_list, \n",
    "                 batch_size_list =batch_size_list, augment_data_list=augment_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the best combinations from 5000 iters with 10,000 iters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch Accuracy: 0.132\n",
      "Training Batch Cost: 2.30255618603\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.356\n",
      "Training Batch Cost: 1.75162571114\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.472\n",
      "Training Batch Cost: 1.55207203588\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.546\n",
      "Training Batch Cost: 1.4265186387\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.534\n",
      "Training Batch Cost: 1.31766765481\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.572\n",
      "Training Batch Cost: 1.25365865113\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.598\n",
      "Training Batch Cost: 1.15116804342\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.632\n",
      "Training Batch Cost: 1.03822685205\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.658\n",
      "Training Batch Cost: 1.01430964353\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.712\n",
      "Training Batch Cost: 0.879469139309\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.68\n",
      "Training Batch Cost: 0.872802181887\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.73\n",
      "Training Batch Cost: 0.84302718806\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.75\n",
      "Training Batch Cost: 0.782434136175\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.772\n",
      "Training Batch Cost: 0.648398186408\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.814\n",
      "Training Batch Cost: 0.600637401204\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.814\n",
      "Training Batch Cost: 0.582098545848\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.804\n",
      "Training Batch Cost: 0.565872693423\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.844\n",
      "Training Batch Cost: 0.504085596371\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.878\n",
      "Training Batch Cost: 0.414279769351\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.83\n",
      "Training Batch Cost: 0.579882036659\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5392\n",
      "Validation Cost: 2.13331806208\n"
     ]
    }
   ],
   "source": [
    "# Momentum with momentum=0.9\n",
    "NN_mom = NeuralNetwork(layer_dimensions, momentum=0.9)\n",
    "NN_mom.train(X_train, y_train, iters=10000, alpha=0.0005, batch_size=500, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch Accuracy: 0.19\n",
      "Training Batch Cost: 2.30255618603\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.422\n",
      "Training Batch Cost: 1.59536979082\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.496\n",
      "Training Batch Cost: 1.45233579815\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.582\n",
      "Training Batch Cost: 1.31261943578\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.578\n",
      "Training Batch Cost: 1.23480210994\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.594\n",
      "Training Batch Cost: 1.22335926902\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.6\n",
      "Training Batch Cost: 1.11990540935\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.63\n",
      "Training Batch Cost: 1.05839372219\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.674\n",
      "Training Batch Cost: 0.988834719043\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.714\n",
      "Training Batch Cost: 0.875626641816\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.718\n",
      "Training Batch Cost: 0.853290324065\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.762\n",
      "Training Batch Cost: 0.792753774976\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.766\n",
      "Training Batch Cost: 0.726175418507\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.758\n",
      "Training Batch Cost: 0.765806855368\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.79\n",
      "Training Batch Cost: 0.617665992255\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.798\n",
      "Training Batch Cost: 0.663401096235\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.782\n",
      "Training Batch Cost: 0.600550720237\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.818\n",
      "Training Batch Cost: 0.572429884461\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.88\n",
      "Training Batch Cost: 0.441194040374\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.84\n",
      "Training Batch Cost: 0.599436272035\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5262\n",
      "Validation Cost: 2.05761288534\n"
     ]
    }
   ],
   "source": [
    "# Adam with momentum=0.9 & decay_rate=0.97\n",
    "NN_adam = NeuralNetwork(layer_dimensions, momentum=0.9, decay_rate=0.97)\n",
    "NN_adam.train(X_train, y_train, iters=10000, alpha=0.0005, batch_size=500, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch Accuracy: 0.132\n",
      "Training Batch Cost: 2.42958450004\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.366\n",
      "Training Batch Cost: 2.18580912398\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.466\n",
      "Training Batch Cost: 2.26255491041\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.55\n",
      "Training Batch Cost: 2.25179259149\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.548\n",
      "Training Batch Cost: 2.40824584826\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.556\n",
      "Training Batch Cost: 2.53545772043\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.646\n",
      "Training Batch Cost: 2.5187530317\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.638\n",
      "Training Batch Cost: 2.60625452664\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.682\n",
      "Training Batch Cost: 2.72753416719\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.674\n",
      "Training Batch Cost: 2.82456547228\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.7\n",
      "Training Batch Cost: 2.91085393037\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.752\n",
      "Training Batch Cost: 2.96668838623\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.738\n",
      "Training Batch Cost: 3.21864447049\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.774\n",
      "Training Batch Cost: 3.22026602815\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.818\n",
      "Training Batch Cost: 3.30975209077\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.83\n",
      "Training Batch Cost: 3.42079126228\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.818\n",
      "Training Batch Cost: 3.57903475833\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.868\n",
      "Training Batch Cost: 3.57613952607\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.86\n",
      "Training Batch Cost: 3.75343761871\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.862\n",
      "Training Batch Cost: 3.89716585269\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5384\n",
      "Validation Cost: 2.10970822326\n"
     ]
    }
   ],
   "source": [
    "# Momentum with L1, reg_lambda=0.2, and momentum=0.9\n",
    "NN_moml1 = NeuralNetwork(layer_dimensions, momentum=0.9, reg_lambda=0.2)\n",
    "NN_moml1.train(X_train, y_train, iters=10000, alpha=0.0005, batch_size=500, print_every=500, reg_type=\"L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch Accuracy: 0.132\n",
      "Training Batch Cost: 2.30278989053\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.36\n",
      "Training Batch Cost: 1.75324544519\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.462\n",
      "Training Batch Cost: 1.58047768398\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.564\n",
      "Training Batch Cost: 1.41012966936\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.562\n",
      "Training Batch Cost: 1.35197887256\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.59\n",
      "Training Batch Cost: 1.2792670906\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.612\n",
      "Training Batch Cost: 1.17207507779\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.648\n",
      "Training Batch Cost: 1.0806613816\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.664\n",
      "Training Batch Cost: 1.02320867627\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.698\n",
      "Training Batch Cost: 1.00386141328\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.704\n",
      "Training Batch Cost: 0.870547086957\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.74\n",
      "Training Batch Cost: 0.865512573225\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.778\n",
      "Training Batch Cost: 0.86441315828\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.758\n",
      "Training Batch Cost: 0.776081656673\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.798\n",
      "Training Batch Cost: 0.703859187644\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.796\n",
      "Training Batch Cost: 0.810891448002\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.818\n",
      "Training Batch Cost: 0.668570492395\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.862\n",
      "Training Batch Cost: 0.563040621863\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.868\n",
      "Training Batch Cost: 0.551221177554\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.844\n",
      "Training Batch Cost: 0.666452110196\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.543\n",
      "Validation Cost: 2.02302516239\n"
     ]
    }
   ],
   "source": [
    "# Momentum with L2, reg_lambda=0.1, and momentum=0.9\n",
    "NN_moml2 = NeuralNetwork(layer_dimensions, momentum=0.9, reg_lambda=0.1)\n",
    "NN_moml2.train(X_train, y_train, iters=10000, alpha=0.0005, batch_size=500, print_every=500, reg_type=\"L2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch Accuracy: 0.188\n",
      "Training Batch Cost: 2.42958450004\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.44\n",
      "Training Batch Cost: 2.54324081285\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.496\n",
      "Training Batch Cost: 2.54858897593\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.596\n",
      "Training Batch Cost: 2.52944759766\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.572\n",
      "Training Batch Cost: 2.65190908441\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.6\n",
      "Training Batch Cost: 2.73903002166\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.602\n",
      "Training Batch Cost: 2.8746374933\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.64\n",
      "Training Batch Cost: 2.86144533351\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.69\n",
      "Training Batch Cost: 2.96919615016\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.696\n",
      "Training Batch Cost: 2.96810679099\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.718\n",
      "Training Batch Cost: 3.03276242599\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.736\n",
      "Training Batch Cost: 3.10924439642\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.764\n",
      "Training Batch Cost: 3.20497275133\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.764\n",
      "Training Batch Cost: 3.25914810845\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.77\n",
      "Training Batch Cost: 3.32093624819\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.8\n",
      "Training Batch Cost: 3.42802138034\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.79\n",
      "Training Batch Cost: 3.46398789655\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.836\n",
      "Training Batch Cost: 3.51862265659\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.852\n",
      "Training Batch Cost: 3.53616691721\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.852\n",
      "Training Batch Cost: 3.64646317523\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5196\n",
      "Validation Cost: 2.04572784712\n"
     ]
    }
   ],
   "source": [
    "# Adam with L1, reg_lambda=0.2, momentum=0.9, decay_rate=0.97\n",
    "NN_adaml1 = NeuralNetwork(layer_dimensions, reg_lambda=0.2, momentum=0.9, decay_rate=0.97)\n",
    "NN_adaml1.train(X_train, y_train, iters=10000, alpha=0.0005, batch_size=500, print_every=500, reg_type=\"L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch Accuracy: 0.19\n",
      "Training Batch Cost: 2.30278989053\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.424\n",
      "Training Batch Cost: 1.61421651017\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.5\n",
      "Training Batch Cost: 1.47612278069\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.566\n",
      "Training Batch Cost: 1.33036313326\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.562\n",
      "Training Batch Cost: 1.29687357607\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.564\n",
      "Training Batch Cost: 1.2457862823\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.6\n",
      "Training Batch Cost: 1.19125179438\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.658\n",
      "Training Batch Cost: 1.08215806221\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.656\n",
      "Training Batch Cost: 1.06609491996\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.72\n",
      "Training Batch Cost: 0.916491221956\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.724\n",
      "Training Batch Cost: 0.919657235272\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.748\n",
      "Training Batch Cost: 0.858783517173\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.74\n",
      "Training Batch Cost: 0.85164013603\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.728\n",
      "Training Batch Cost: 0.814979689869\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.748\n",
      "Training Batch Cost: 0.756038702554\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.812\n",
      "Training Batch Cost: 0.708907619875\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.774\n",
      "Training Batch Cost: 0.70130255839\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.792\n",
      "Training Batch Cost: 0.658824625174\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.824\n",
      "Training Batch Cost: 0.614985053441\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.824\n",
      "Training Batch Cost: 0.658660032357\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5206\n",
      "Validation Cost: 1.96692300774\n"
     ]
    }
   ],
   "source": [
    "# Adam with L2, reg_lambda=0.1, momentum=0.9, decay_rate=0.97\n",
    "NN_adaml2 = NeuralNetwork(layer_dimensions, reg_lambda=0.1, momentum=0.9, decay_rate=0.97)\n",
    "NN_adaml2.train(X_train, y_train, iters=10000, alpha=0.0005, batch_size=500, print_every=500, reg_type=\"L2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch Accuracy: 0.186\n",
      "Training Batch Cost: 2.3025557013\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.398\n",
      "Training Batch Cost: 1.70482198051\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.518\n",
      "Training Batch Cost: 1.45866627706\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.544\n",
      "Training Batch Cost: 1.44526188238\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.546\n",
      "Training Batch Cost: 1.3310626344\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.566\n",
      "Training Batch Cost: 1.40267300701\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.582\n",
      "Training Batch Cost: 1.33552731451\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.566\n",
      "Training Batch Cost: 1.3111122936\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.62\n",
      "Training Batch Cost: 1.25392215271\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.616\n",
      "Training Batch Cost: 1.23852719543\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.634\n",
      "Training Batch Cost: 1.17965877799\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.67\n",
      "Training Batch Cost: 1.0910963555\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.64\n",
      "Training Batch Cost: 1.1896011233\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.656\n",
      "Training Batch Cost: 1.1266737617\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.664\n",
      "Training Batch Cost: 1.08389611717\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.69\n",
      "Training Batch Cost: 1.10690498722\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.666\n",
      "Training Batch Cost: 1.03533951381\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.67\n",
      "Training Batch Cost: 1.09977308298\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.674\n",
      "Training Batch Cost: 1.02205354612\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.74\n",
      "Training Batch Cost: 0.921545033941\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5536\n",
      "Validation Cost: 1.5290518114\n"
     ]
    }
   ],
   "source": [
    "# Adam with Dropout, drop_prob=0.2, momentum=0.9, decay_rate=0.97\n",
    "NN_adamdrop = NeuralNetwork(layer_dimensions, drop_prob=0.2, momentum=0.9, decay_rate=0.97)\n",
    "NN_adamdrop.train(X_train, y_train, iters=10000, alpha=0.0005, batch_size=500, print_every=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Summary\n",
    "\n",
    "### Best version is: Adam with Dropout, drop_prob=0.2, momentum=0.9, and decay_rate=0.97\n",
    "### Accuracy on a validation set: 0.5536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch Accuracy: 0.186\n",
      "Training Batch Cost: 2.3025557013\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.398\n",
      "Training Batch Cost: 1.70482198051\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.518\n",
      "Training Batch Cost: 1.45866627706\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.544\n",
      "Training Batch Cost: 1.44526188238\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.546\n",
      "Training Batch Cost: 1.3310626344\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.566\n",
      "Training Batch Cost: 1.40267300701\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.582\n",
      "Training Batch Cost: 1.33552731451\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.566\n",
      "Training Batch Cost: 1.3111122936\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.62\n",
      "Training Batch Cost: 1.25392215271\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.616\n",
      "Training Batch Cost: 1.23852719543\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.634\n",
      "Training Batch Cost: 1.17965877799\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.67\n",
      "Training Batch Cost: 1.0910963555\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.64\n",
      "Training Batch Cost: 1.1896011233\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.656\n",
      "Training Batch Cost: 1.1266737617\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.664\n",
      "Training Batch Cost: 1.08389611717\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.69\n",
      "Training Batch Cost: 1.10690498722\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.666\n",
      "Training Batch Cost: 1.03533951381\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.67\n",
      "Training Batch Cost: 1.09977308298\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.674\n",
      "Training Batch Cost: 1.02205354612\n",
      "--------------------------\n",
      "Training Batch Accuracy: 0.74\n",
      "Training Batch Cost: 0.921545033941\n",
      "--------------------------\n",
      "##########################\n",
      "--------------------------\n",
      "Validation Accuracy: 0.5536\n",
      "Validation Cost: 1.5290518114\n"
     ]
    }
   ],
   "source": [
    "### Best version:\n",
    "# Adam with Dropout, drop_prob=0.2, momentum=0.9, decay_rate=0.97\n",
    "NN2 = NeuralNetwork(layer_dimensions, drop_prob=0.2, momentum=0.9, decay_rate=0.97)\n",
    "NN2.train(X_train, y_train, iters=10000, alpha=0.0005, batch_size=500, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, 4, 5, 1, 8, 4, 8, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted2 = NN2.predict(X_test)\n",
    "save_predictions('ans2-sy2569', y_predicted2)\n",
    "\n",
    "# check if it's saved:\n",
    "loaded_y = np.load('ans2-sy2569.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "|                                           |              |          |          | NN Init   |            |          |            | train |        |            |             |          |              |\n",
    "|-------------------------------------------|--------------|----------|----------|-----------|------------|----------|------------|-------|--------|------------|-------------|----------|--------------|\n",
    "| Note                                      | Accuracy     | Opt      | Reg      | drop_prob | reg_lambda | momentum | decay_rate | iters | alpha  | batch_size | print_every | reg_type | augment_data |\n",
    "| Part 1                                    | 0.5114       | N/A      | N/A      | 0         | 0          | 0        | 0          | 10000 | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 1                                    | 0.494        | N/A      | N/A      | 0         | 0          | 0        | 0          | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.5002       | N/A      | L1       | 0         | 0.05       | 0        | 0          | 5000  | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.4796       | N/A      | L1       | 0         | 0.1        | 0        | 0          | 5000  | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.5102       | N/A      | L1       | 0         | 0.2        | 0        | 0          | 5000  | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.4922       | N/A      | L1       | 0         | 0.3        | 0        | 0          | 5000  | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.5156       | N/A      | L1       | 0         | 0.2        | 0        | 0          | 10000 | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.5082       | N/A      | L2       | 0         | 0.1        | 0        | 0          | 5000  | 0.0005 | 500        | 500         | L2       | FALSE        |\n",
    "| Part 2                                    | 0.486        | N/A      | L2       | 0         | 0.2        | 0        | 0          | 5000  | 0.0005 | 500        | 500         | L2       | FALSE        |\n",
    "| Part 2                                    | 0.5198       | N/A      | L2       | 0         | 0.5        | 0        | 0          | 5000  | 0.0005 | 500        | 500         | L2       | FALSE        |\n",
    "| Part 2                                    | 0.5076       | N/A      | Dropout  | 0.1       | 0          | 0        | 0          | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.5172       | N/A      | Dropout  | 0.2       | 0          | 0        | 0          | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.514        | N/A      | Dropout  | 0.3       | 0          | 0        | 0          | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.4023333333 | N/A      | Data Aug | 0         | 0          | 0        | 0          | 5000  | 0.0005 | 500        | 500         | N/A      | TRUE         |\n",
    "| Part 2                                    | 0.549        | Momentum | N/A      | 0         | 0          | 0.9      | 0          | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.5026       | RMS      | N/A      | 0         | 0          | 0        | 0.9        | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.492        | RMS      | N/A      | 0         | 0          | 0        | 0.97       | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.5392       | Adam     | N/A      | 0         | 0          | 0.9      | 0.97       | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.5302       | Adam     | N/A      | 0         | 0          | 0.9      | 0.99       | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.5362       | Momentum | L1       | 0         | 0.2        | 0.9      | 0          | 5000  | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.5434       | Momentum | L2       | 0         | 0.1        | 0.9      | 0          | 5000  | 0.0005 | 500        | 500         | L2       | FALSE        |\n",
    "| Part 2                                    | 0.5284       | Momentum | L2       | 0         | 0.5        | 0.9      | 0          | 5000  | 0.0005 | 500        | 500         | L2       | FALSE        |\n",
    "| Part 2                                    | 0.5288       | Momentum | Dropout  | 0.2       | 0          | 0.9      | 0          | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.455        | Momentum | Data Aug | 0         | 0          | 0.9      | 0          | 5000  | 0.0005 | 500        | 500         | N/A      | TRUE         |\n",
    "| Part 2                                    | 0.5056       | RMS      | L1       | 0         | 0.2        | 0        | 0.9        | 5000  | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.5104       | RMS      | L2       | 0         | 0.1        | 0        | 0.9        | 5000  | 0.0005 | 500        | 500         | L2       | FALSE        |\n",
    "| Part 2                                    | 0.5018       | RMS      | Dropout  | 0.2       | 0          | 0        | 0.9        | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.4366666667 | RMS      | Data Aug | 0         | 0          | 0        | 0.9        | 5000  | 0.0005 | 500        | 500         | N/A      | TRUE         |\n",
    "| Part 2                                    | 0.5284       | Adam     | L1       | 0         | 0.2        | 0.9      | 0.9        | 5000  | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.5354       | Adam     | L1       | 0         | 0.2        | 0.9      | 0.95       | 5000  | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.5402       | Adam     | L1       | 0         | 0.2        | 0.9      | 0.97       | 5000  | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.5382       | Adam     | L1       | 0         | 0.2        | 0.9      | 0.98       | 5000  | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.517        | Adam     | L2       | 0         | 0.1        | 0.9      | 0.9        | 5000  | 0.0005 | 500        | 500         | L2       | FALSE        |\n",
    "| Part 2                                    | 0.5332       | Adam     | L2       | 0         | 0.1        | 0.9      | 0.97       | 5000  | 0.0005 | 500        | 500         | L2       | FALSE        |\n",
    "| Part 2                                    | 0.5238       | Adam     | Dropout  | 0.2       | 0          | 0.9      | 0.9        | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.5316       | Adam     | Dropout  | 0.2       | 0          | 0.9      | 0.97       | 5000  | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.4636666667 | Adam     | Data Aug | 0         | 0          | 0.9      | 0.97       | 5000  | 0.0005 | 500        | 500         | N/A      | TRUE         |\n",
    "| Try 10,000 iters for the best ones above: |              |          |          |           |            |          |            |       |        |            |             |          |              |\n",
    "| Part 2                                    | 0.5392       | Momentum | N/A      | 0         | 0          | 0.9      | 0          | 10000 | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.5262       | Adam     | N/A      | 0         | 0          | 0.9      | 0.97       | 10000 | 0.0005 | 500        | 500         | N/A      | FALSE        |\n",
    "| Part 2                                    | 0.5384       | Momentum | L1       | 0         | 0.2        | 0.9      | 0          | 10000 | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.543        | Momentum | L2       | 0         | 0.1        | 0.9      | 0          | 10000 | 0.0005 | 500        | 500         | L2       | FALSE        |\n",
    "| Part 2                                    | 0.5196       | Adam     | L1       | 0         | 0.2        | 0.9      | 0.97       | 10000 | 0.0005 | 500        | 500         | L1       | FALSE        |\n",
    "| Part 2                                    | 0.5206       | Adam     | L2       | 0         | 0.1        | 0.9      | 0.97       | 10000 | 0.0005 | 500        | 500         | L2       | FALSE        |\n",
    "| Part 2                                    | 0.5536       | Adam     | Dropout  | 0.2       | 0          | 0.9      | 0.97       | 10000 | 0.0005 | 500        | 500         | N/A      | FALSE        |"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
